{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiGT9J2VkUA88juT/Gu475"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "dqcS3fY5OnpP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAyYAO1eOduO",
        "outputId": "9d8a241e-6041-47b4-9eff-3e46ffd374c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.62.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (3.10.15)\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cloudpickle~=2.2.1 (from apache-beam)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam)\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache-beam)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 (from apache-beam)\n",
            "  Downloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (4.23.0)\n",
            "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam)\n",
            "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=1.14.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (1.26.4)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam)\n",
            "  Downloading objsize-0.7.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (24.2)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam)\n",
            "  Downloading pymongo-4.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (4.25.6)\n",
            "Collecting pydot<2,>=1.2.0 (from apache-beam)\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (2024.2)\n",
            "Collecting redis<6,>=5.0.0 (from apache-beam)\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (2024.11.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (2.32.3)\n",
            "Collecting sortedcontainers>=2.4.0 (from apache-beam)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (4.12.2)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (0.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /usr/local/lib/python3.11/dist-packages (from apache-beam) (6.0.2)\n",
            "Collecting pyarrow<17.0.0,>=3.0.0 (from apache-beam)\n",
            "  Downloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix<1 (from apache-beam)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.17.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam) (3.2.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.22.3)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2024.12.14)\n",
            "Downloading apache_beam-2.62.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objsize-0.7.1-py3-none-any.whl (11 kB)\n",
            "Downloading pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Downloading pymongo-4.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: crcmod, dill, hdfs, docopt\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp311-cp311-linux_x86_64.whl size=31657 sha256=054537e89de95b528d8b5374f939b24def730ebf3018c52d4c42957c6cf61ca5\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=913d26bfb8a3d8340e13e9ca744443ad4a5d9661a0c4afc08f931e1caf642355\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/60/80/1622338bcecce31a5664ef01c203cc5a7b09f59588d9c07376\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=abdeb457598eaf999fa597a3806c17760a9f4986384c07819b6c35ce6d97c121\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/1d/dc/eb0833be25464c359903d356c4204721c6a672c26ff164cdc3\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=b93d4b43b609172bcf73f40454ea0dcec15522301919399b427a0b72ae60baef\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built crcmod dill hdfs docopt\n",
            "Installing collected packages: sortedcontainers, docopt, crcmod, redis, pydot, pyarrow-hotfix, pyarrow, objsize, jsonpickle, grpcio, fasteners, fastavro, dnspython, dill, cloudpickle, pymongo, hdfs, apache-beam\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 3.0.4\n",
            "    Uninstalling pydot-3.0.4:\n",
            "      Successfully uninstalled pydot-3.0.4\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "  Attempting uninstall: jsonpickle\n",
            "    Found existing installation: jsonpickle 4.0.1\n",
            "    Uninstalling jsonpickle-4.0.1:\n",
            "      Successfully uninstalled jsonpickle-4.0.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.70.0\n",
            "    Uninstalling grpcio-1.70.0:\n",
            "      Successfully uninstalled grpcio-1.70.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.1\n",
            "    Uninstalling cloudpickle-3.1.1:\n",
            "      Successfully uninstalled cloudpickle-3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2024.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.62.0 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 dnspython-2.7.0 docopt-0.6.2 fastavro-1.10.0 fasteners-0.19 grpcio-1.65.5 hdfs-2.7.3 jsonpickle-3.4.2 objsize-0.7.1 pyarrow-16.1.0 pyarrow-hotfix-0.6 pydot-1.4.2 pymongo-4.11 redis-5.2.1 sortedcontainers-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install apache-beam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##exclude_ids.txt"
      ],
      "metadata": {
        "id": "NzFv5GxAw3VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "149633CM\n",
        "212539MU\n",
        "231555ZZ\n",
        "704275DC"
      ],
      "metadata": {
        "id": "Ln2hh6aYwwuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Side Inputs"
      ],
      "metadata": {
        "id": "dMhOC7oZwf1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Co to jest side input?\n",
        "\n",
        "Jak sama nazwa wskazuje, jest to dodatkowa informacja, którą możemy przekazać do obiektu DoFn.\n",
        "\n",
        "Oprócz głównej PCollection jako wejścia, możemy wstrzyknąć dodatkowe dane do ParDo lub jego pochodnych transformacji, takich jak Map i FlatMap, w formie side inputs.\n",
        "\n",
        "ParDo traktuje side input jako dodatkowe wejście, do którego ma dostęp za każdym razem, gdy przetwarza element w PCollection."
      ],
      "metadata": {
        "id": "NTXYqL4MmDEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam  # Importujemy Apache Beam do budowy potoku przetwarzania danych\n",
        "\n",
        "# Tworzymy pustą listę, w której przechowamy identyfikatory pracowników do wykluczenia\n",
        "side_list = list()\n",
        "\n",
        "# Otwieramy plik 'exclude_ids.txt', który zawiera listę ID do wykluczenia\n",
        "with open('exclude_ids.txt', 'r') as my_file:\n",
        "    for line in my_file:\n",
        "        # Usuwamy białe znaki (np. '\\n' z końca każdej linii) i dodajemy ID do listy\n",
        "        side_list.append(line.rstrip())\n",
        "\n",
        "# Tworzymy obiekt potoku Beam\n",
        "p = beam.Pipeline()\n",
        "\n",
        "# Definiujemy klasę DoFn, która będzie używana w transformacji ParDo do filtrowania danych\n",
        "class FilterUsingLength(beam.DoFn):\n",
        "    def process(self, element, side_list, lower_bound, upper_bound=float('inf')):\n",
        "        \"\"\"\n",
        "        Metoda process() wykonuje operację filtrowania na każdym elemencie PCollection.\n",
        "        - element: pojedynczy wiersz wejściowych danych jako string.\n",
        "        - side_list: lista ID do wykluczenia (przekazana jako side input).\n",
        "        - lower_bound: dolna granica długości imienia.\n",
        "        - upper_bound: górna granica długości imienia.\n",
        "        \"\"\"\n",
        "\n",
        "        # Rozdzielamy wiersz wejściowy na poszczególne wartości, zakładając format CSV\n",
        "        id = element.split(',')[0]  # Pobieramy pierwszą kolumnę (ID pracownika)\n",
        "        name = element.split(',')[1]  # Pobieramy drugą kolumnę (Imię pracownika)\n",
        "\n",
        "        # Usuwamy ewentualne błędy kodowania znaków\n",
        "        # id = id.decode('utf-8', 'ignore').encode(\"utf-8\")\n",
        "        id = id.encode(\"utf-8\")\n",
        "\n",
        "        # Tworzymy listę elementów, które później zwrócimy, jeśli spełnią warunki\n",
        "        element_list = element.split(',')\n",
        "\n",
        "        # Sprawdzamy, czy długość imienia mieści się w podanym przedziale oraz\n",
        "        # czy ID nie znajduje się na liście wykluczonych pracowników\n",
        "        if (lower_bound <= len(name) <= upper_bound) and id not in side_list:\n",
        "            return [element_list]  # Zwracamy tylko poprawne wiersze\n",
        "\n",
        "# Definiujemy przetwarzanie danych w potoku Beam\n",
        "small_names = (\n",
        "    p\n",
        "    | \"Read from text file\" >> beam.io.ReadFromText('dept_data.txt')  # Wczytujemy dane wejściowe z pliku tekstowego\n",
        "    | \"ParDo with side inputs\" >> beam.ParDo(FilterUsingLength(), side_list, 3, 10)  # Filtrowanie za pomocą ParDo i side inputs\n",
        "    | beam.Filter(lambda record: record[3] == 'Accounts')  # Zachowujemy tylko pracowników z działu 'Accounts'\n",
        "    | beam.Map(lambda record: (record[0] + \" \" + record[1], 1))  # Mapujemy ID + imię jako klucz, wartość = 1\n",
        "    | beam.CombinePerKey(sum)  # Sumujemy wartości dla każdego unikalnego klucza (ID + imię)\n",
        "    | 'Write results' >> beam.io.WriteToText('output_new_final')  # Zapisujemy wyniki do pliku wyjściowego\n",
        ")\n",
        "\n",
        "# Uruchamiamy potok\n",
        "p.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "936ZHKlGCzN7",
        "outputId": "81d5a180-135a-4332-817d-9e677fa7cda2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7db81cb23a90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{('head -n 20 /content/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a1xH8RRFF1i",
        "outputId": "615a22d7-40c4-41db-b990-1d5588f8a75b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('149633CM Marco', 31)\n",
            "('212539MU Rebekah', 31)\n",
            "('231555ZZ Itoe', 31)\n",
            "('503996WI Edouard', 31)\n",
            "('704275DC Kyle', 31)\n",
            "('957149WC Kyle', 31)\n",
            "('241316NX Kumiko', 31)\n",
            "('796656IE Gaston', 31)\n",
            "('718737IX Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dodatkowe wyjścia (additional outputs)"
      ],
      "metadata": {
        "id": "jjHlRq72HwqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do tej pory widzieliśmy, że transformacja zwraca tylko jedno główne wyjście `PCollection`. Jednak możliwe jest wygenerowanie wielu dodatkowych wyjściowych `PCollection` obok głównego. Jeśli zdecydujemy się na wiele wyjść, nasza transformacja zwróci wszystkie kolekcje `PCollection` razem w pakiecie. Aby emitować elementy do wielu wyjściowych `PCollection`, używamy funkcji `with_outputs()` w transformacji ParDo i określamy różne tagi dla wyjść.\n",
        "\n",
        "Przykład – filtrowanie imion według różnych warunków. Rozważmy potok, który odczytuje plik departamentów.\n",
        "\n",
        "Chcemy podzielić dane na trzy grupy:\n",
        "1.\tKrótkie imiona – imiona o długości do 4 znaków.\n",
        "2.\tDługie imiona – imiona o długości powyżej 4 znaków.\n",
        "3.\tImiona zaczynające się na “A”.\n",
        "\n",
        "Ten przypadek można rozwiązać na dwa sposoby:\n",
        "1.\tPoprzez rozgałęzienie potoku (`branched pipelines`) – wymagałoby to więcej kodu.\n",
        "2.\tPoprzez dodatkowe wyjścia (`additional outputs`), co jest bardziej eleganckie."
      ],
      "metadata": {
        "id": "bTOeKVjYLsfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam  # Importujemy Apache Beam\n",
        "\n",
        "# Definiujemy klasę ProcessWords dziedziczącą po beam.DoFn, aby przetwarzać dane w potoku.\n",
        "class ProcessWords(beam.DoFn):\n",
        "    def process(self, element, cutoff_length, marker):\n",
        "        \"\"\"\n",
        "        Funkcja process() jest wywoływana dla każdego elementu w PCollection.\n",
        "        Przetwarza wiersz wejściowy i przydziela go do odpowiednich kategorii.\n",
        "\n",
        "        :param element: pojedynczy wiersz danych wejściowych (np. \"101,Anna,HR\")\n",
        "        :param cutoff_length: maksymalna długość krótkiego imienia (np. 4)\n",
        "        :param marker: litera, na którą ma zaczynać się imię (np. \"A\")\n",
        "        :return: odpowiednio otagowane wartości PCollection\n",
        "        \"\"\"\n",
        "        name = element.split(',')[1]  # Pobieramy drugą kolumnę, czyli imię\n",
        "\n",
        "        if len(name) <= cutoff_length:\n",
        "            # Jeśli długość imienia jest mniejsza lub równa cutoff_length, zwracamy je jako Short_Names\n",
        "            return [beam.pvalue.TaggedOutput('Short_Names', name)]\n",
        "\n",
        "        else:\n",
        "            # W przeciwnym razie zwracamy je jako Long_Names\n",
        "            return [beam.pvalue.TaggedOutput('Long_Names', name)]\n",
        "\n",
        "        if name.startswith(marker):\n",
        "            # Jeśli imię zaczyna się na określony znak (np. 'M'), zwracamy je jako główne wyjście\n",
        "            return name\n",
        "\n",
        "# Tworzymy potok Apache Beam\n",
        "p = beam.Pipeline()\n",
        "\n",
        "# Przetwarzamy dane wejściowe\n",
        "results = (\n",
        "    p\n",
        "    | \"Read File\" >> beam.io.ReadFromText('/content/dept_data.txt')  # Odczytujemy dane wejściowe z pliku dept_data.txt\n",
        "    | \"Process Names\" >> beam.ParDo(ProcessWords(), cutoff_length=4, marker='M')\n",
        "        .with_outputs('Short_Names', 'Long_Names', main='Names_M')  # Definiujemy tagi dla dodatkowych wyjść\n",
        ")\n",
        "\n",
        "# Przypisujemy poszczególne kolekcje do zmiennych na podstawie tagów\n",
        "short_collection = results.Short_Names  # PCollection z krótkimi imionami\n",
        "long_collection = results.Long_Names  # PCollection z długimi imionami\n",
        "startM_collection = results.Names_M  # PCollection z imionami zaczynającymi się na \"M\"\n",
        "\n",
        "# Zapisujemy wyniki do plików\n",
        "short_collection | 'Write Short Names' >> beam.io.WriteToText('short')\n",
        "long_collection | 'Write Long Names' >> beam.io.WriteToText('long')\n",
        "startM_collection | 'Write Names Starting With M' >> beam.io.WriteToText('start_m')\n",
        "\n",
        "# Uruchamiamy potok\n",
        "p.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKkeTZxi3NyI",
        "outputId": "592e47bd-9519-404c-9a3e-a1d56a77447f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7f54af331650>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2-73aNT5LvM",
        "outputId": "28e2ec90-3c72-4d3d-cebd-9fab2c5da228"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dept_data.txt  long-00000-of-00001  \u001b[0m\u001b[01;34msample_data\u001b[0m/  short-00000-of-00001  start_m-00000-of-00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{('head -n 5 long-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7W5ktPm44u6",
        "outputId": "4ff15a0d-8337-43ae-e94f-9743acad8970"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marco\n",
            "Rebekah\n",
            "Edouard\n",
            "Kumiko\n",
            "Gaston\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{('head -n 5 short-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlFygXAw5VQb",
        "outputId": "1d656d5a-ca98-4b9a-abac-88dffb22d411"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itoe\n",
            "Kyle\n",
            "Kyle\n",
            "Olga\n",
            "Kirk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{('head -n 5 start_m-00000-of-00001')}"
      ],
      "metadata": {
        "id": "eUJFXxhf5gSQ"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}